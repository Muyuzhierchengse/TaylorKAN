{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TaylorKAN + CNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TaylorLayer(nn.Module):\n",
    "  def __init__(self, input_dim, out_dim, order, addbias=True):\n",
    "    super(TaylorLayer, self).__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.out_dim = out_dim\n",
    "    self.order = order\n",
    "    self.addbias = addbias\n",
    "\n",
    "    # 初始化泰勒系数\n",
    "    self.coeffs = nn.Parameter(torch.randn(out_dim, input_dim, order) * 0.01)  # 维度: (outdim, inputdim, order)\n",
    "    if self.addbias:\n",
    "      self.bias = nn.Parameter(torch.zeros(1, out_dim))  # 维度: (1, outdim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    shape = x.shape\n",
    "    outshape = shape[0:-1] + (self.out_dim,)\n",
    "    x = torch.reshape(x, (-1, self.input_dim))  # 重塑x为二维张量，形状: (batch_size, inputdim)\n",
    "\n",
    "    # 扩展x以便与coeffs维度对齐\n",
    "    x_expanded = x.unsqueeze(1).expand(-1, self.out_dim, -1)  # 形状: (batch_size, outdim, inputdim)\n",
    "\n",
    "    # 计算泰勒展开的每一项并累加\n",
    "    y = torch.zeros((x.shape[0], self.out_dim), device=x.device)  # 初始化输出 (batch_size, outdim)\n",
    "\n",
    "    for i in range(self.order):\n",
    "      term = (x_expanded ** i) * self.coeffs[:, :, i]  # 计算第i阶项并乘以相应系数\n",
    "      y += term.sum(dim=-1)  # 将第i阶项的每个inputdim的贡献相加\n",
    "\n",
    "    if self.addbias:\n",
    "      y += self.bias  # 加上偏置\n",
    "\n",
    "    y = torch.reshape(y, outshape)\n",
    "    return y\n",
    "\n",
    "class TaylorCNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TaylorCNN, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "    self.pool1 = nn.MaxPool2d(2)\n",
    "    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "    self.pool2 = nn.MaxPool2d(2)\n",
    "    self.taylorkan1 = TaylorLayer(32*7*7, 128, 2)\n",
    "    self.taylorkan2 = TaylorLayer(128, 10, 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # 卷积\n",
    "    x = F.selu(self.conv1(x))\n",
    "    # 池化\n",
    "    x = self.pool1(x)\n",
    "    # 卷积\n",
    "    x = F.selu(self.conv2(x))\n",
    "    # 池化\n",
    "    x = self.pool2(x)\n",
    "    # 将特征图展平成二维向量\n",
    "    x = x.view(x.size(0), -1)\n",
    "    # KAN层\n",
    "    x = self.taylorkan1(x)\n",
    "    # KAN层\n",
    "    x = self.taylorkan2(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Dataset and DataLoader setup\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "# subset_indices = np.random.choice(len(train_dataset), int(len(train_dataset) * 0.1), replace=False)\n",
    "# train_subset = Subset(train_dataset, subset_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Model, Optimizer and Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TaylorCNN().to(device)\n",
    "# optimizer = optim.LBFGS(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-4, momentum=0.9) # 似乎只能使用LBFGS优化器\n",
    "optimizer = optim.RAdam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "  model.train()\n",
    "  for i, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data.to(device))\n",
    "    loss = nn.CrossEntropyLoss()(output, target.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "      print(f'Train Epoch: {epoch} [{i * len(data)}/{len(train_loader.dataset)} ({100. * i / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, device, test_loader):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      output = model(data)\n",
    "      test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
    "\n",
    "# Running training and evaluation\n",
    "for epoch in range(0, 2):\n",
    "  train(model, device, train_loader, optimizer, epoch)\n",
    "evaluate(model, device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
